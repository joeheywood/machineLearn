install.packages("plyr")
require(plyr)
install.packages("shiny")
install.packages('ggplot2')
install.packages("zoo")
require(zoo)
q()
q()
require(data.table)
install.packages("data.table")
require('data.table')
vignette('data.table-intro')
vignette('data.table')
vignette('datatable-intro')
ldf <- data.frame(firstCol=rnorm(1000000), secondCol=rbinom(1000000))
?rbinom
ldf <- data.frame(firstCol=rnorm(1000000), secondCol=rnorm(1000000))
dim(ldf)
?microbenchmark
install.packages("microbenchmark")
library(microbenchmark)
data()
DT1 <- data.table(letter=letters, v1=1:26)
DT2 <- data.table(letter=letters, v1=1:26)
DF1 <- data.frame(letter=letter, v1=1:26)
DF2 <- data.frame(letter=letter, v1=1:26)
microbenchmark(merge(DT1, DT2, by='letter'), merge(DF1, DF2, by='letter'))
DT1 <- data.table(letter=letters, v1=1:26)
DT2 <- data.table(letter=letters, v1=1:26)
DF1 <- data.frame(letter=letters, v1=1:26)
DF2 <- data.frame(letter=letters, v1=1:26)
microbenchmark(merge(DT1, DT2, by='letter'), merge(DF1, DF2, by='letter'))
microbenchmark(mergeFrame(DF1, DF2), mergeFrame(DT1, DT2))
mergeSpeed <- function(N) {
ID = expand.grid(list(a=letters, b=letters, c=letters))
ID <- paste(ID$a, ID$b, ID$c)
ID = ID[1:N]
mergeFrame <- function(DF1, DF2){
ind1 = sample(1:nrow(DF1))
ind2 = sample(1:nrow(DF2))
merge(DF1[ind1, ], DF2[ind2,], by = 'ID')
}
DF1 <- data.frame(ID=ID, x1 = rnorm(length(ID)))
DF2 <- data.frame(ID=ID, x1 = rnorm(length(ID)))
DT1 <- as.data.table(DF1)
DT2 <- as.data.frame(DF2)
microbenchmark(mergeFrame(DF1, DF2), mergeFrame(DT1, DT2))
}
a<- lapply(c(1000,10000,100000), mergeSpeed)
mergeSpeed(100)
mergeSpeed(1000)
mergeSpeed(10000)
mergeSpeed(100000)
a<- lapply(c(100, 1000,10000,17576), mergeSpeed)
a[[4]]
a[[1]]
a[[2]]
require(ggplot2)
str(a[[1]])
a[[1]]$time
str(a[[1]])
a[[1]]$expr
a[[1]]
table(a[[1]])
a[[1]]
a[[1]]
adf <- data.frame(df=1.225, dt=2.419)
a[[2]]
adf <- data.frame(df=1.225, dt=2.419, n=100)
tapply(a[[1]]$time, a[[1]]$expr, FUN = median)
t =tapply(a[[1]]$time, a[[1]]$expr, FUN = median)
class(t)
t[1]
t =lapply(a[[1]]$time, a[[1]]$expr, FUN = median)
class(t)
t[[1]]
t[[2]]
t
t =lapply(1:5, function(x){a[[x]]$time, a[[x]]$expr, FUN = median})
tl =lapply(1:5, function(x){tapply(a[[x]]$time, a[[x]]$expr, FUN = median)})
tl =lapply(1:4, function(x){tapply(a[[x]]$time, a[[x]]$expr, FUN = median)})
tl
tld <- do.call(rbind, tl)
tld
class(tld)
tldf <- data.frame(tld)
tldf
colnames(tldf) = c('Data frame', 'data.table')
tldf
ggplot(tldf) + geom_line(aes(x=))
require(reshape2)
m <- melt(tldf)
m
m$n = c(100, 1000, 10000, 17576, 100, 1000, 10000, 17576)
m
ggplot(m) + geom_bar(aes(y=value, x=n, fill=variable))
ggplot(m) + geom_bar(aes(y=value, x=n, fill=variable), stat=identity)
ggplot(m) + geom_bar(aes(y=value, x=n, fill=variable), stat='identity')
ggplot(m) + geom_bar(aes(y=value, x=n, fill=variable), stat='identity', position=dodge)
ggplot(m) + geom_line(aes(y=value, x=n, colour=variable), stat='identity')
ggplot(m) + geom_line(aes(y=log(value), x=n, colour=variable), stat='identity')
mergeSpeed <- function(N) {
ID = expand.grid(list(a=letters, b=letters, c=letters))
ID <- paste(ID$a, ID$b, ID$c)
ID = ID[1:N]
mergeFrame <- function(DF1, DF2){
ind1 = sample(1:nrow(DF1))
ind2 = sample(1:nrow(DF2))
merge(DF1[ind1, ], DF2[ind2,], by = 'ID')
}
DF1 <- data.frame(ID=ID, x1 = rnorm(length(ID)))
DF2 <- data.frame(ID=ID, x1 = rnorm(length(ID)))
DT1 <- as.data.table(DF1)
DT2 <- as.data.frame(DF2)
DTK1 <- DT1
DTK2 <- DT2
setkey(DTK1, ID)
setkey(DTK1, ID)
microbenchmark(mergeFrame(DF1, DF2), mergeFrame(DT1, DT2), mergeFrame(DTK1, DTK2))
}
mergeSpeed(1000)
mergeSpeed(10000)
ggplot(m) + geom_line(aes(y=log(value), x=n, colour=variable), stat='identity') + xlab("Number of rows (log, expand.grid")
ggplot(m) + geom_line(aes(y=log(value), x=n, colour=variable), stat='identity') + xlab("Number of rows (expand.grid")
ggplot(m) + geom_line(aes(y=log(value), x=n, colour=variable), stat='identity') + xlab("Number of rows") + ylab("")
q()
source('speedComparison.R')
bmks <- run()
require(data.table)
bmks <- run()
require(microbenchmark)
bmks <- run()
require(microbenchmark)
bmks <- run()
source('speedComparison.R')
bmks <- run()
str(bmks)
source('speedComparison.R')
tt <- run()
head(tt)
ttdf <-data.frame(tt)
ttdf$rows<- c(100, 1000, 10000, 17576)
ttdf
colnames(ttdf) <- c('data.frame', 'data.table', 'rows')
ttdf
require(reshape2)
m <- melt(ttdf, id.vars=rows)
m <- melt(ttdf, id.vars='rows')
m
ggplot(m + geom_line(aes(x=rows, y=value, colour=variable)))
require(ggplot2)
ggplot(m + geom_line(aes(x=rows, y=value, colour=variable)))
ggplot(m) + geom_line(aes(x=rows, y=value, colour=variable))
ggplot(m) + geom_line(aes(x=rows, y=log(value), colour=variable))
source('speedComparison.R')
run()
source('speedComparison.R')
run()
ttdf
m <- melt(ttdf, id.vars='rows')
m
ggplot(m) + geom_line(aes(x=rows, y=log(value), colour=variable))
source('speedComparison.R')
r<-run()
r
str(r)
r+xlab("fasdf")
source('speedComparison.R')
r<-run()
source('speedComparison.R')
r<-run()
source('speedComparison.R')
r<-run()
r
q()
q()
q()
install.packages('AppliedPRedictiveModelling')
library(caret)
library(tcltk)
install.packages('tcltk')
?Corpus
require(tm)
?Corpus
f <- readRDS('Desktop/council_spending/leeds/rds/leeds201101.rds')
colnames(f)
ff <- f$`desc: Expendtiure category`
ff <- f$`desc: Expenditure category`
corp <- Corpus(VectorSource(ff))
corp[2]
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords('english'))
dt <- TermDocumentMatrix(corp)
dt <- TermDocumentMatrix(ff, control=list(removePunctuation=T, stopwords=T))
corp <- Corpus(DataframeSource(ff))
corp <- Corpus(DataframeSource(data.frame(f=ff))
)
dt <- TermDocumentMatrix(corp)
corp <- tm_map(corp, tolower)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeWords, stopwords('english'))
dt <- TermDocumentMatrix(corp)
corp <- Corpus(VectorSource(ff))
corp <- tm_map(corp, tolower)
dt <- TermDocumentMatrix(corp)
corp <- Corpus(VectorSource(tolower(ff)))
dt <- TermDocumentMatrix(corp)
findFreqTerms(dt, lowfreq=30)
dt
dt
dt$dimnames
dt$nrow
dt$col
dt$ncol
removeWords(corp, stopwords("english"))
removeWords(ff, stopwords("english"))
fff<- removeWords(tolower(ff), stopwords("english"))
corp <- Corpus(VectorSource(tolower(fff)))
dt <- TermDocumentMatrix(corp)
findFreqTerms(dt, lowfreq=30)
findFreqTerms(dt, lowfreq=10)
q()
a <- read.csv('Desktop/council_spending/leeds/leedsSectors.csv')
head(a)
table(a$sector)
a <- read.csv('Desktop/council_spending/leeds/leedsSectors.csv', stringsAsFactors=F)
root <- 'Desktop/council_spending/leeds/rds/'
fls <- list.files(root)
fls <- paste0(root, fls)
fls[3]
b <- do.call(rbind.fill, lapply(fls, readRDS ))
require(plyr)
b <- do.call(rbind.fill, lapply(fls, readRDS ))
head(b)
colnames(b)
b < - b[,c(6,8,10)]
b <- b[,c(6,8,10)]
head(b)
b$cat <- b[,1]
head(b)
head(b[is.na(b$cat),])
b[is.na(b$cat), 'cat'] <- b[is.na(b$cat), 3]
nrow(b[is.na(b$cat),])
b <- b[,c(2, 4)]
head(b)
colnames(b)[1] <- 'supplier'
View(a)
colnames(a)
a <- [,c(2,8)]
a <- a[,c(2,8)]
c<- merge(a, b)
head(c)
head(a)
head(b)
c<- merge(a, b, by='supplier')
head(c)
c[45:55,]
c <- join(a, b, by='supplier', type='right')
head(c)
table(c$sector)
c <- c[,2:3]
require(caret)
intr <- createDataPartition(c$sector)
c$sector <- factor(c$sector, levels=c('vol', 'other'))
head(c)
c <- join(a, b, by='supplier', type='right')
colnames(c)
c$sector <- as.factor(c$sector, levels=c('vol', 'other'))
str(c)
c$sector2 <- factor(c$sector, levels=c('vol', 'other'))
head(c)
c$sector2 <- Factor(c$sector, levels=c('vol', 'other'))
c$sector2 <- as.factor(c$sector, levels=c('vol', 'other'))
c$sector2 <- as.factor(c$sector, labels=c('vol', 'other')
head(c)
c$sector2 <- as.factor(c$sector, labels=c('vol', 'other'))
c$sector2 <- as.factor(c$sector)
head(C)
head(c)
str(c)
c$sector2 <- factor(c$sector)
c$sector2 <- factor(c$sector, labels=c('vol', 'oth'))
head(c)
c <- c[,c(3,4)]
source('Desktop/tm_testing.R')
source('Desktop/tm_testing.R')
bb <- buildCorpus(c$cat)
findFreqTerms(bb, lowfreq=30)
in <- createDataPartition(c$sector2)
intrain <- createDataPartition(c$sector2)
intrain <- createDataPartition(c$sector2, list=F)
intrain <- createDataPartition(c$sector2, list=F, p=0.6)
bb <- buildCorpus(c[intrain,])
head(intrain)
nrow(c[intrain,])
findFreqTerms(bb, lowfreq=30)
training <- c[intrain,]
head(training)
bb <- buildCorpus(training$cat)
model <- naiveBayes(as.matrix(bb),training$sector2);
install.packages('e1071')
require(e1071)
model <- naiveBayes(as.matrix(bb),training$sector2);
nrow(as.matrix(bb))
nrow(as.matrix(t(bb)))
model <- naiveBayes(as.matrix(t(bb)),training$sector2);
testing <- c[-intrain,]
cc <- buildCorpus(testing$cat)
results <- predict(model,as.matrix(t(cc)));
str(results)
table(results)
q()
dir.create('machineLearn')
setwd('machineLearn/')
dt <- read.csv('pml-training.csv', stringsAsFactors=FALSE)
head(dt)
j <- colnames(dt)
dt
j
q <- grepl('stddev', j)
j[q]
head(dt[,j[q]])
head(dt)
q <- grepl('(stddev|skew)', j)
head(dt[,j[q]])
head(dt[,-(j[q]]))
head(dt[,-(j[q])])
head(dt[,-j[q]])
head(dt[,j[q]])
head(dt[,!j[q]])
head(dt[,-(j[q])])
head(dt[,!(j[q])])
head(dt[,j[-q]])
q <- grepl('(stddev|skew|avg)', j)
head(dt[,j[-q]])
q <- grepl('^(stddev|skew|avg)', j)
j[q]
j[-q]
q <- grepl('^(stddev|skewness|avg)', j)
j[-q]
j[q]
j[-(q)]
j[!q]
head(dt[,j[!q]])
q <- grepl('^(stddev|skewness|avg|kurtosis)', j)
q <- grepl('^(stddev|skewness|avg|kurtosis|min|max)', j)
j[!q]
head(dt[,j[!q]])
q <- grepl('^(stddev|skewness|avg|kurtosis|min|max|var)', j)
j[!q]
head(dt[,j[!q]])
q <- grepl('^(stddev|skewness|avg|kurtosis|min|max|var|amplitude)', j)
head(dt[,j[!q]])
j[!q]
trimmedVars <- j[!q]
trimmedVars
trimmedVars <- trimmedVars[-1:7]
trimmedVars <- trimmedVars[-(1:7)]
trimmedVars
head(dt[,trimmedVars])
dt <- getData()
source('assignment.R')
source('assignment.R')
dt <- getData()
require(caret)
?caretFuncs
?createDataPartition
inTrain <- createDataPartition(dt$classe, p=0.6)
training <- dt[inTrain,]
testing <- dt[-inTrain,]
modFit <- train(classe~., data=training[1:500], method='rtf', prox=TRUE)
inTrain <- createDataPartition(dt$classe, p=0.6, list=FALSE)
training <- dt[inTrain,]
testing <- dt[-inTrain,]
modFit <- train(classe~., data=training[1:500], method='rtf', prox=TRUE)
View(testing)
dt$classe <- factor(dt$classe)
modFit <- train(classe~., data=training[1:500], method='rtf', prox=TRUE)
training <- dt[inTrain,]
testing <- dt[-inTrain,]
modFit <- train(classe~., data=training[1:500], method='rtf', prox=TRUE)
modFit <- train(classe~., data=training[1:500,], method='rtf', prox=TRUE)
modFit <- train(classe~., data=training[1:500,], method='rf', prox=TRUE)
require(randomForest)
modFit <- train(classe~., data=training[1:500,], method='rf', prox=TRUE)
warnings()
modFit <- train(classe~., data=training[1:500,], method='nb', prox=TRUE)
?train
modFit <- train(classe~., data=training, weights=NULL, method='nb', prox=TRUE)
modFit <- train(classe~., data=training, weights=NULL, method='rf', prox=TRUE, subset=100)
modFit <- train(classe~., data=training, weights=NULL, method='rf', prox=TRUE, subset=1:100)
modFit <- train(classe~., data=training, weights=NULL, method='nb', prox=TRUE, subset=1:100)
warnings
warnings()
head(training)
str(training)
source('assignment.R')
dt <- getData()
inTrain <- createDataPartition(dt$classe, p=0.6, list=FALSE)
training <- dt[inTrain,]
testing <- dt[-inTrain,]
dt <- read.csv('pml-training.csv', stringsAsFactors=FALSE)
# now get rid of all the variables we don't need
blanks <- grepl('^(stddev|skewness|avg|kurtosis|min|max|var|amplitude|total)', j)
trimmedVars <- j[!blanks]
# more metadata variables at beginning not really needed.
trimmedVars <- trimmedVars[-(1:7)]
dt <- dt[,trimmedVars]
# save for future use.
saveRDS(dt, file='trimmed.rds')
dt <- getData()
inTrain <- createDataPartition(dt$classe, p=0.6, list=FALSE)
training <- dt[inTrain,]
testing <- dt[-inTrain,]
head(training)
modFit <- train(classe~., data=training, weights=NULL, method='rf', prox=TRUE, subset=1:500)
warnings()
summary(training)
modFit <- train(classe~., data=training, weights=NULL, method='rf', prox=TRUE, subset=1:3000)
modFit <- train(classe~., data=training, weights=NULL, method='nb', prox=TRUE, subset=1:300)
modFit <- train(classe~., data=training, weights=NULL, method='rf', prox=TRUE, subset=1:300)
q()
